{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the transformation pipeline is to prepare the data in a way that it can be easily consumed by your downstream processes, whether that's a machine learning model, a recommendation system, or any other analytics\n",
    "\n",
    "For the first stage, our primary goal is to develop a beta version of the recommendation system that recommends a job title based on job_qualifications, job_type, and job_location. We are not currently focusing on job_summary and job_description.\n",
    "\n",
    "We'll focus on ensuring the job_qualifications, job_type, and job_location columns are appropriately processed and transformed for the recommendation system.\n",
    "\n",
    "1. Datetime Conversion and Feature Extraction:\n",
    "\n",
    "    a. Convert the date_of_job_post column into a datetime format.\n",
    "    b. Extract the month, day, and year from the date_of_job_post and save them as separate columns. This can help in any time-based analysis or features you might want to consider in the future.\n",
    "\n",
    "2. Categorical Encoding:\n",
    "\n",
    "a. Convert categorical columns (job_location, company_name, job_type, and especially the title as it's your target) to a numerical format.\n",
    "    \n",
    "    a(1). Use appropriate encoding techniques. For title, which is the primary recommendation target, a label encoding might be sufficient. For other features like job_location or job_type, you might consider one-hot encoding if the number of unique categories is not too high, or label encoding otherwise.\n",
    "\n",
    "3. Handling Missing Values:\n",
    "\n",
    "    a. Ensure there are no missing values in your crucial columns like job_qualifications, job_type, and job_location. Decide on a strategy to handle them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update config.yaml entry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration entity for the Data Transformation pipeline.\n",
    "    \n",
    "    This configuration entity provides necessary paths and directories \n",
    "    related to data transformation activities.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir (Path): The root directory where data transformation artifacts are stored.\n",
    "    - data_path (Path): The path to the dataset (typically CSV) that needs to be transformed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Root directory for storing transformation-related artifacts\n",
    "    root_dir: Path\n",
    "\n",
    "    # Path to the validated dataset for transformation\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConfigurationManager configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pixi_hr.constants import *\n",
    "from src.pixi_hr.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath=CONFIG_FILE_PATH,    # Path to the main configuration YAML file\n",
    "            params_filepath=PARAMS_FILE_PATH,    # Path to the parameters YAML file\n",
    "            schema_filepath=SCHEMA_FILE_PATH):   # Path to the schema YAML file\n",
    "        \n",
    "        # Load configurations, parameters, and schema details from their respective YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        # Create root directory for storing all artifacts, as specified in the configuration\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extracts the data transformation configuration from the main config.\n",
    "\n",
    "        The function performs the following steps:\n",
    "        1. Reads the data transformation section of the configuration.\n",
    "        2. Ensures the specified root directory for data transformation exists (creates it if not).\n",
    "        3. Returns a DataTransformationConfig object initialized with the extracted configuration.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: A dataclass object containing the data transformation configuration.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract the data transformation configuration from the main config\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        # Ensure the specified root directory for data transformation exists (creates it if not)\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Create an instance of the DataTransformationConfig dataclass using the extracted configuration\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pixi_hr import logger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    DataTransformation class for transforming the raw data into a machine learning ready format.\n",
    "    \n",
    "    Attributes:\n",
    "    - config (DataTransformationConfig): Configuration object containing paths.\n",
    "    - df (DataFrame): Pandas DataFrame loaded from the specified data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation class.\n",
    "\n",
    "        Args:\n",
    "        - config (DataTransformationConfig): Configuration object containing paths.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        try:\n",
    "            # Load the data into a DataFrame\n",
    "            self.df = pd.read_csv(self.config.data_path)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading data file: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def clean_skills(self, skill_list_str):\n",
    "        \"\"\"Clean the skills from the job_qualifications column.\"\"\"\n",
    "        # Convert the string representation of a list to an actual list\n",
    "        skill_list = eval(skill_list_str)\n",
    "\n",
    "        # Clean each skill\n",
    "        cleaned_skills = []\n",
    "        for skill in skill_list:\n",
    "            cleaned_skill = skill.strip().lower()  # Convert to lowercase and remove leading/trailing whitespaces\n",
    "            cleaned_skill = re.sub(r'[^a-z0-9]', '', cleaned_skill)  # Remove special characters\n",
    "            cleaned_skills.append(cleaned_skill)\n",
    "\n",
    "        return cleaned_skills\n",
    "    \n",
    "    def one_hot_encode_qualifications(self):\n",
    "        \"\"\"One-hot encodes the job qualifications.\"\"\"\n",
    "        # Clean the skills\n",
    "        self.df['job_qualifications'] = self.df['job_qualifications'].apply(self.clean_skills)\n",
    "\n",
    "        # Initialize the MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer()\n",
    "\n",
    "        # One-hot encode the cleaned skills\n",
    "        encoded_qualifications = mlb.fit_transform(self.df['job_qualifications'])\n",
    "\n",
    "        # Add a prefix to the encoded column names\n",
    "        encoded_columns = [f\"qual_{col}\" for col in mlb.classes_]\n",
    "\n",
    "        # Convert the one-hot encoded skills into a DataFrame\n",
    "        encoded_df = pd.DataFrame(encoded_qualifications, columns=encoded_columns)\n",
    "\n",
    "        # Drop the original job_qualifications column and concat the encoded DataFrame\n",
    "        self.df = pd.concat([self.df.drop('job_qualifications', axis=1), encoded_df], axis=1)\n",
    "\n",
    "        logger.info(\"One-hot encoding of job qualifications completed.\")\n",
    "\n",
    "\n",
    "    \n",
    "    def datetime_conversion_and_extraction(self):\n",
    "        \"\"\"\n",
    "        Convert the date_of_job_post column into a datetime format and extract\n",
    "        year, month, day, hour, minute, and second as separate columns.\n",
    "        \"\"\"\n",
    "        self.df['date_of_job_post'] = pd.to_datetime(self.df['date_of_job_post'])\n",
    "        \n",
    "        # Extracting different datetime features\n",
    "        self.df['month_of_job_post'] = self.df['date_of_job_post'].dt.month\n",
    "        self.df['day_of_job_post'] = self.df['date_of_job_post'].dt.day\n",
    "        self.df['year_of_job_post'] = self.df['date_of_job_post'].dt.year\n",
    "        self.df['hour_of_job_post'] = self.df['date_of_job_post'].dt.hour\n",
    "        self.df['minute_of_job_post'] = self.df['date_of_job_post'].dt.minute\n",
    "        self.df['second_of_job_post'] = self.df['date_of_job_post'].dt.second\n",
    "\n",
    "        logger.info(\"Datetime conversion and feature extraction completed.\")\n",
    "    \n",
    "   \n",
    "    def categorical_encoding(self):\n",
    "        \"\"\"\n",
    "        Convert categorical columns to numerical format using label encoding.\n",
    "        \"\"\"\n",
    "        label_encoders = {}  # Store the encoders for potential use later\n",
    "        for col in ['title', 'job_location', 'company_name', 'job_type']:\n",
    "            le = LabelEncoder()\n",
    "            self.df[col] = le.fit_transform(self.df[col])\n",
    "            label_encoders[col] = le\n",
    "        logger.info(\"Categorical encoding completed.\")\n",
    "\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        Handle missing values in the specified columns by dropping them.\n",
    "        \"\"\"\n",
    "        self.df.dropna(subset=['job_qualifications', 'job_type', 'job_location'], inplace=True)\n",
    "        logger.info(\"Handling of missing values completed.\")\n",
    "\n",
    "    \n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Split the data into train and test sets and save them to respective paths.\n",
    "        \"\"\"\n",
    "        train, test = train_test_split(self.df, test_size=0.2, random_state=44)\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, 'train_data.csv'), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, 'test_data.csv'), index=False)\n",
    "\n",
    "        logger.info(\"Data split into train and test sets and saved to respective paths.\")\n",
    "        logger.info(f\"Train shape: {train.shape}\")\n",
    "        logger.info(f\"Test shape: {test.shape}\")\n",
    "\n",
    "        print(f\"Train shape: {train.shape}\")\n",
    "        print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the sequence of data transformations.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting Data Transformation...\")\n",
    "        \n",
    "        # Step 1: Datetime Conversion and Feature Extraction\n",
    "        logger.info(\"Datetime Conversion and Feature Extraction...\")\n",
    "        self.datetime_conversion_and_extraction()\n",
    "\n",
    "        # Step 2: Categorical Encoding\n",
    "        logger.info(\"Categorical Encoding...\")\n",
    "        self.categorical_encoding()\n",
    "\n",
    "        # Step 3: Handling Missing Values\n",
    "        logger.info(\"Handling Missing Values...\")\n",
    "        self.handle_missing_values()\n",
    "\n",
    "        # Step 4: One-Hot Encoding of Job Qualifications\n",
    "        logger.info(\"One-Hot Encoding of Job Qualifications...\")\n",
    "        self.one_hot_encode_qualifications()\n",
    "\n",
    "        # Step 5: Split Data into Train and Test sets\n",
    "        logger.info(\"Splitting Data...\")\n",
    "        self.split_data()\n",
    "\n",
    "        logger.info(\"Data Transformation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-26 13:30:57,397: 37: pixi_hr_project_logger: INFO: 231967474:  >>>>>> Stage: Data Transformation Stage started <<<<<<]\n",
      "[2023-08-26 13:30:57,398: 14: pixi_hr_project_logger: INFO: 231967474:  Starting the Data Transformation Pipeline]\n",
      "[2023-08-26 13:30:57,399: 17: pixi_hr_project_logger: INFO: 231967474:  Initializing configuration manager]\n",
      "[2023-08-26 13:30:57,403: 41: pixi_hr_project_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-08-26 13:30:57,405: 41: pixi_hr_project_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-08-26 13:30:57,406: 41: pixi_hr_project_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-08-26 13:30:57,407: 64: pixi_hr_project_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-08-26 13:30:57,407: 21: pixi_hr_project_logger: INFO: 231967474:  Fetching Data Transformation Configuration..]\n",
      "[2023-08-26 13:30:57,408: 64: pixi_hr_project_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2023-08-26 13:30:57,408: 25: pixi_hr_project_logger: INFO: 231967474:  Initializing Data Transformation Component...]\n",
      "[2023-08-26 13:30:57,437: 29: pixi_hr_project_logger: INFO: 231967474:  Transforming data...]\n",
      "[2023-08-26 13:30:57,437: 127: pixi_hr_project_logger: INFO: 2698079030:  Starting Data Transformation...]\n",
      "[2023-08-26 13:30:57,438: 130: pixi_hr_project_logger: INFO: 2698079030:  Datetime Conversion and Feature Extraction...]\n",
      "[2023-08-26 13:30:57,450: 83: pixi_hr_project_logger: INFO: 2698079030:  Datetime conversion and feature extraction completed.]\n",
      "[2023-08-26 13:30:57,451: 134: pixi_hr_project_logger: INFO: 2698079030:  Categorical Encoding...]\n",
      "[2023-08-26 13:30:57,454: 95: pixi_hr_project_logger: INFO: 2698079030:  Categorical encoding completed.]\n",
      "[2023-08-26 13:30:57,454: 138: pixi_hr_project_logger: INFO: 2698079030:  Handling Missing Values...]\n",
      "[2023-08-26 13:30:57,458: 103: pixi_hr_project_logger: INFO: 2698079030:  Handling of missing values completed.]\n",
      "[2023-08-26 13:30:57,459: 142: pixi_hr_project_logger: INFO: 2698079030:  One-Hot Encoding of Job Qualifications...]\n",
      "[2023-08-26 13:30:57,471: 64: pixi_hr_project_logger: INFO: 2698079030:  One-hot encoding of job qualifications completed.]\n",
      "[2023-08-26 13:30:57,472: 146: pixi_hr_project_logger: INFO: 2698079030:  Splitting Data...]\n",
      "[2023-08-26 13:30:57,510: 115: pixi_hr_project_logger: INFO: 2698079030:  Data split into train and test sets and saved to respective paths.]\n",
      "[2023-08-26 13:30:57,510: 116: pixi_hr_project_logger: INFO: 2698079030:  Train shape: (174, 168)]\n",
      "[2023-08-26 13:30:57,511: 117: pixi_hr_project_logger: INFO: 2698079030:  Test shape: (44, 168)]\n",
      "Train shape: (174, 168)\n",
      "Test shape: (44, 168)\n",
      "[2023-08-26 13:30:57,511: 149: pixi_hr_project_logger: INFO: 2698079030:  Data Transformation completed successfully.]\n",
      "[2023-08-26 13:30:57,512: 32: pixi_hr_project_logger: INFO: 231967474:  Data Transformation Pipeline completed successfully.]\n",
      "[2023-08-26 13:30:57,512: 40: pixi_hr_project_logger: INFO: 231967474:  >>>>>> Stage Data Transformation Stage completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "from pixi_hr import logger\n",
    "from pixi_hr.config.configuration import ConfigurationManager \n",
    "from pixi_hr.components.data_transformation import DataTransformationConfig\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "\n",
    "    STAGE_NAME = \"Data Transformation Stage\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        logger.info(\"Starting the Data Transformation Pipeline\")\n",
    "\n",
    "        # Step 1: Initialize Configuration Manager\n",
    "        logger.info(\"Initializing configuration manager\")\n",
    "        config = ConfigurationManager()\n",
    "\n",
    "        # Step 2: Fetch Data Transformation Configuration\n",
    "        logger.info(\"Fetching Data Transformation Configuration..\")\n",
    "        data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "        # Step 3: Initialize Data Transformation Component\n",
    "        logger.info(\"Initializing Data Transformation Component...\")\n",
    "        data_transformation = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "        # Step 4: Transform Data\n",
    "        logger.info(\"Transforming data...\")\n",
    "        data_transformation.main()\n",
    "\n",
    "        logger.info(\"Data Transformation Pipeline completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> Stage: {DataTransformationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "        data_transformation_pipeline = DataTransformationPipeline()\n",
    "        data_transformation_pipeline.main()\n",
    "        logger.info(f\">>>>>> Stage {DataTransformationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error encountered during the Data Transformation Pipeline: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_hr_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

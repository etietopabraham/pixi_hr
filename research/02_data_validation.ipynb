{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data validation phase, the primary goal is to ensure that the dataset meets the expected quality standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update config.yaml, and update the schema for balidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pixi_hr.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_of_job_post</th>\n",
       "      <th>title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_qualifications</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-14T17:00:35Z</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Cineplex</td>\n",
       "      <td>https://www.simplyhired.ca/job/tsfSmO_biXPolNH...</td>\n",
       "      <td>Lead the data/IT governance process and manage...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>['Power BI', 'SQL', 'Tableau', 'MTA', 'Data vi...</td>\n",
       "      <td>Work location:\\nHome Office 1303 Yonge St\\n\\nC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-13T01:10:36Z</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Greater Toronto Area, ON</td>\n",
       "      <td>fgf brands</td>\n",
       "      <td>https://www.simplyhired.ca/job/dnDqxJfbA0BNLi3...</td>\n",
       "      <td>We are seeking a talented and experienced Data...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>['Analysis skills', 'TensorFlow', 'Communicati...</td>\n",
       "      <td>Job Description:\\nData Scientist – Facial Reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-06T22:18:35Z</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mississauga, ON</td>\n",
       "      <td>Procom</td>\n",
       "      <td>https://www.simplyhired.ca/job/A-L5E5YUGKQWdur...</td>\n",
       "      <td>Data mining or extracting usable data from val...</td>\n",
       "      <td>Contract</td>\n",
       "      <td>['SQL', 'Microsoft Excel', 'SAS', 'NoSQL', 'Ca...</td>\n",
       "      <td>On behalf of our client in the Transportation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-06T03:12:09Z</td>\n",
       "      <td>Staff Data Scientist</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>ISG Search Inc</td>\n",
       "      <td>https://www.simplyhired.ca/job/-SAKV7mxY8l7tfm...</td>\n",
       "      <td>Lead a small team of data scientists. Collabor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['TensorFlow', 'C++', 'Azure', 'C', 'Machine l...</td>\n",
       "      <td>Must Have:\\n7+ years of experience with genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-05T08:44:07Z</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Canada Life Assurance Company</td>\n",
       "      <td>https://www.simplyhired.ca/job/Tiz9efu8Gbf2yqV...</td>\n",
       "      <td>Make data-driven business recommendations and ...</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>['TensorFlow', 'Power BI', 'Communication skil...</td>\n",
       "      <td>Job Description:\\nCanada Life is seeking a hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_of_job_post                 title               job_location  \\\n",
       "0  2023-07-14T17:00:35Z        Data Scientist               Toronto, ON    \n",
       "1  2023-07-13T01:10:36Z        Data Scientist  Greater Toronto Area, ON    \n",
       "2  2023-07-06T22:18:35Z        Data Scientist           Mississauga, ON    \n",
       "3  2023-07-06T03:12:09Z  Staff Data Scientist               Toronto, ON    \n",
       "4  2023-07-05T08:44:07Z        Data Scientist               Toronto, ON    \n",
       "\n",
       "                    company_name  \\\n",
       "0                       Cineplex   \n",
       "1                     fgf brands   \n",
       "2                         Procom   \n",
       "3                 ISG Search Inc   \n",
       "4  Canada Life Assurance Company   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.simplyhired.ca/job/tsfSmO_biXPolNH...   \n",
       "1  https://www.simplyhired.ca/job/dnDqxJfbA0BNLi3...   \n",
       "2  https://www.simplyhired.ca/job/A-L5E5YUGKQWdur...   \n",
       "3  https://www.simplyhired.ca/job/-SAKV7mxY8l7tfm...   \n",
       "4  https://www.simplyhired.ca/job/Tiz9efu8Gbf2yqV...   \n",
       "\n",
       "                                         job_summary   job_type  \\\n",
       "0  Lead the data/IT governance process and manage...  Full-time   \n",
       "1  We are seeking a talented and experienced Data...  Full-time   \n",
       "2  Data mining or extracting usable data from val...   Contract   \n",
       "3  Lead a small team of data scientists. Collabor...        NaN   \n",
       "4  Make data-driven business recommendations and ...  Full-time   \n",
       "\n",
       "                                  job_qualifications  \\\n",
       "0  ['Power BI', 'SQL', 'Tableau', 'MTA', 'Data vi...   \n",
       "1  ['Analysis skills', 'TensorFlow', 'Communicati...   \n",
       "2  ['SQL', 'Microsoft Excel', 'SAS', 'NoSQL', 'Ca...   \n",
       "3  ['TensorFlow', 'C++', 'Azure', 'C', 'Machine l...   \n",
       "4  ['TensorFlow', 'Power BI', 'Communication skil...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Work location:\\nHome Office 1303 Yonge St\\n\\nC...  \n",
       "1  Job Description:\\nData Scientist – Facial Reco...  \n",
       "2  On behalf of our client in the Transportation ...  \n",
       "3  Must Have:\\n7+ years of experience with genera...  \n",
       "4  Job Description:\\nCanada Life is seeking a hig...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"artifacts/data_ingestion/jobs_data_simply_hired.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   date_of_job_post    239 non-null    object\n",
      " 1   title               239 non-null    object\n",
      " 2   job_location        239 non-null    object\n",
      " 3   company_name        239 non-null    object\n",
      " 4   job_link            239 non-null    object\n",
      " 5   job_summary         239 non-null    object\n",
      " 6   job_type            170 non-null    object\n",
      " 7   job_qualifications  239 non-null    object\n",
      " 8   job_description     239 non-null    object\n",
      "dtypes: object(9)\n",
      "memory usage: 16.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 218 entries, 0 to 217\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   date_of_job_post       218 non-null    object\n",
      " 1   title                  218 non-null    object\n",
      " 2   job_location           218 non-null    object\n",
      " 3   company_name           218 non-null    object\n",
      " 4   job_link               218 non-null    object\n",
      " 5   job_summary            218 non-null    object\n",
      " 6   job_type               157 non-null    object\n",
      " 7   job_qualifications     218 non-null    object\n",
      " 8   job_description        218 non-null    object\n",
      " 9   date_of_job_post_temp  218 non-null    object\n",
      "dtypes: object(10)\n",
      "memory usage: 17.2+ KB\n"
     ]
    }
   ],
   "source": [
    "validated_df = pd.read_csv(\"artifacts/data_validation/validated_jobs_data.csv\")\n",
    "validated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity (Update config.yaml first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Using the dataclass decorator to create a class that mainly represents a data structure.\n",
    "# The frozen=True parameter makes the instances of this dataclass immutable.\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    # Path to the root directory where data validation artifacts are stored.\n",
    "    root_dir: Path\n",
    "    \n",
    "    # Location of the extracted data (in this case, a CSV file) that needs to be validated.\n",
    "    unzip_data_dir: Path\n",
    "    \n",
    "    # Path to a status file used to track the progress or status of data validation.\n",
    "    STATUS_FILE: str\n",
    "\n",
    "    # Store all schema configuration\n",
    "    all_schema: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pixi_hr.constants import *\n",
    "from src.pixi_hr.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,   # Path to the configuration YAML file\n",
    "            params_filepath = PARAMS_FILE_PATH,   # Path to the parameters YAML file\n",
    "            schema_filepath = SCHEMA_FILE_PATH):  # Path to the schema YAML file\n",
    "        \n",
    "        # Load the configuration details from the YAML file\n",
    "\n",
    "        # print(type(config_filepath), config_filepath)\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        # Load the parameters details from the YAML file\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        # Load the schema details from the YAML file\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "\n",
    "        # Create directories as specified in the configuration (e.g., for storing artifacts)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        # Extract data validation configuration from the main configuration\n",
    "        config = self.config.data_validation\n",
    "        # Extract schema columns from schema.yaml\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        # Create directories specified in the data valition configuration\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Create an instance of the DataValidationConfig dataclass using the extracted configuration\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            unzip_data_dir=config.unzip_data_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            all_schema=schema\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast  # Required for the validate_job_qualifications method\n",
    "from pixi_hr import logger\n",
    "from pixi_hr.entity.config_entity import DataValidationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    Data Validation class to ensure data quality and integrity.\n",
    "    \n",
    "    Attributes:\n",
    "    - config (DataValidationConfig): Configuration object containing paths and schema information.\n",
    "    - df (DataFrame): Pandas DataFrame loaded from the specified data file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidation class.\n",
    "\n",
    "        Args:\n",
    "        - config (DataValidationConfig): Configuration object containing paths and schema information.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        try:\n",
    "            # Load the data into a DataFrame\n",
    "            self.df = pd.read_csv(self.config.unzip_data_dir)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.unzip_data_dir}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading data file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def validate_columns(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if all expected columns are present in the dataset.\n",
    "        Logs missing or extra columns and writes the validation status \n",
    "        to a specified file.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all columns are as expected, False otherwise.\n",
    "        \"\"\"\n",
    "        # Initialize the validation status and the status message\n",
    "        validation_status = True\n",
    "        status_message = \"Validation status: \"\n",
    "        \n",
    "        # Determine missing or extra columns\n",
    "        all_columns = set(self.df.columns)\n",
    "        expected_columns = set(self.config.all_schema.keys())\n",
    "        missing_columns = expected_columns - all_columns\n",
    "        extra_columns = all_columns - expected_columns\n",
    "        \n",
    "        # Log and update the status message for any discrepancies\n",
    "        if missing_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Missing columns: {', '.join(missing_columns)}\")\n",
    "            status_message += f\"Missing columns: {', '.join(missing_columns)}\\n\"\n",
    "        if extra_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Extra columns found: {', '.join(extra_columns)}\")\n",
    "            status_message += f\"Extra columns found: {', '.join(extra_columns)}\\n\"\n",
    "        if validation_status:\n",
    "            logger.info(\"All expected columns are present in the dataframe.\")\n",
    "            status_message += \"All expected columns are present.\"\n",
    "        \n",
    "        # Write the validation status to the file\n",
    "        self._write_status_to_file(status_message)\n",
    "\n",
    "        return validation_status\n",
    "\n",
    "    def _write_status_to_file(self, message: str):\n",
    "        \"\"\"\n",
    "        Writes a given message to the status file specified in the config.\n",
    "\n",
    "        Args:\n",
    "        - message (str): The message to write.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                f.write(message)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to status file: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def validate_date_of_job_post(self):\n",
    "        \"\"\"\n",
    "        Validate that 'date_of_job_post' contains valid date-time strings.\n",
    "        Logs any discrepancies.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df['date_of_job_post_temp'] = pd.to_datetime(self.df['date_of_job_post'], errors='raise')\n",
    "            logger.info(\"All values in 'date_of_job_post' are valid date-time strings.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error encountered: {e}\")\n",
    "            self.df['date_of_job_post_temp'] = pd.to_datetime(self.df['date_of_job_post'], errors='coerce')\n",
    "            invalid_rows = self.df[self.df['date_of_job_post_temp'].isna()]\n",
    "            logger.warning(\"\\nRows with invalid date-time strings:\")\n",
    "            logger.warning(invalid_rows[['date_of_job_post']])\n",
    "\n",
    "    \n",
    "    def validate_text_fields(self, columns):\n",
    "        \"\"\"\n",
    "        Validate that the specified columns contain only text values.\n",
    "        Logs any columns containing non-text values.\n",
    "\n",
    "        Args:\n",
    "        - columns (list): List of column names to validate.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            non_text_rows = self.df[self.df[column].apply(lambda x: not isinstance(x, str))]\n",
    "            if not non_text_rows.empty:\n",
    "                logger.warning(f\"Column '{column}' has non-text values:\")\n",
    "                logger.warning(non_text_rows[[column]])\n",
    "            else:\n",
    "                logger.info(f\"Column '{column}' contains only text values.\")\n",
    "\n",
    "    \n",
    "    def validate_job_link(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_link' starts with \"http\".\n",
    "        Logs any URLs not starting with 'http'.\n",
    "        \"\"\"\n",
    "        invalid_urls = self.df[~self.df['job_link'].str.startswith(\"http\")]\n",
    "        if not invalid_urls.empty:\n",
    "            logger.warning(f\"Found {len(invalid_urls)} rows with URLs not starting with 'http':\")\n",
    "            logger.warning(invalid_urls[['job_link']])\n",
    "        else:\n",
    "            logger.info(\"All URLs in 'job_link' start with 'http'.\")\n",
    "\n",
    "\n",
    "    def validate_job_type(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_type' contains only text values, excluding nulls.\n",
    "        Logs any non-text values found.\n",
    "        \"\"\"\n",
    "        non_null_job_types = self.df[self.df['job_type'].notna()]\n",
    "        non_text_job_types = non_null_job_types[~non_null_job_types['job_type'].apply(lambda x: isinstance(x, str))]\n",
    "        if not non_text_job_types.empty:\n",
    "            logger.warning(f\"Found {len(non_text_job_types)} rows in 'job_type' with non-text values:\")\n",
    "            logger.warning(non_text_job_types[['job_type']])\n",
    "        else:\n",
    "            logger.info(\"All non-null values in 'job_type' are of text type.\")\n",
    "\n",
    "    \n",
    "    def validate_job_qualifications(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_qualifications' contains valid lists of text values.\n",
    "        Logs any invalid format found.\n",
    "        \"\"\"\n",
    "        def is_valid_list(value):\n",
    "            try:\n",
    "                lst = ast.literal_eval(value)\n",
    "                return isinstance(lst, list) and all(isinstance(i, str) for i in lst)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return False\n",
    "\n",
    "        invalid_qualifications = self.df[~self.df['job_qualifications'].apply(is_valid_list)]\n",
    "        if not invalid_qualifications.empty:\n",
    "            logger.warning(f\"Found {len(invalid_qualifications)} rows in 'job_qualifications' with invalid format:\")\n",
    "            logger.warning(invalid_qualifications[['job_qualifications']])\n",
    "        else:\n",
    "            logger.info(\"All values in 'job_qualifications' are valid lists of text values.\")\n",
    "\n",
    "\n",
    "    def handle_duplicates(self):\n",
    "        \"\"\"\n",
    "        Handle duplicate rows based on the 'job_link' column.\n",
    "        Logs the number of duplicates found and handled.\n",
    "        \"\"\"\n",
    "        num_duplicates = self.df[self.df['job_link'].duplicated()].shape[0]\n",
    "        if num_duplicates > 0:\n",
    "            self.df.drop_duplicates(subset='job_link', inplace=True)\n",
    "            logger.info(f\"Dropped {num_duplicates} duplicate rows based on the 'job_link' column.\")\n",
    "            self._save_dataframe()\n",
    "        else:\n",
    "            logger.info(\"No duplicates found based on the 'job_link' column.\")\n",
    "\n",
    "    \n",
    "    def _save_dataframe(self):\n",
    "        \"\"\"\n",
    "        Save the dataframe to the output path specified in the configuration.\n",
    "\n",
    "        I think it will be best practice to save the updated data frame in \n",
    "        our data validation artifacts directory, and work with only the validated data.\n",
    "\n",
    "        That's a prudent approach. There are a few reasons why this makes sense:\n",
    "\n",
    "        Data Provenance: Saving the validated data separately ensures that you always \n",
    "        have a trace of how your data has changed at each step of your pipeline. \n",
    "        This is especially crucial for auditing, reproducing results, or troubleshooting issues down the line.\n",
    "\n",
    "        Data Integrity: By keeping the original data untouched, you have a fallback. \n",
    "        If there's ever a concern about the validation or transformation process, \n",
    "        you can always revert to the original dataset and re-run your validations.\n",
    "\n",
    "        Efficiency: Once the data has been validated, future processes or pipelines can \n",
    "        directly use the validated data without having to re-run the validation steps, \n",
    "        saving time and computational resources.\n",
    "\n",
    "        Clarity: For team members or other collaborators, having clearly demarcated datasets \n",
    "        (raw vs. validated) can make understanding the data processing pipeline much clearer. \n",
    "        They can easily identify which dataset to use based on the stage of analysis or modeling they are in.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df.to_csv(self.config.validated_data_file, index=False)\n",
    "            logger.info(f\"Data saved successfully to {self.config.validated_data_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while saving the dataframe: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-15 20:32:47,895: 10: pixi_hr_project_logger: INFO: 369255095:  Starting the Data Validation Pipeline...]\n",
      "[2023-08-15 20:32:47,897: 13: pixi_hr_project_logger: INFO: 369255095:  Initializing ConfigurationManager...]\n",
      "<class 'pathlib.PosixPath'> config/config.yaml\n",
      "[2023-08-15 20:32:47,899: 41: pixi_hr_project_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2023-08-15 20:32:47,900: 41: pixi_hr_project_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2023-08-15 20:32:47,901: 41: pixi_hr_project_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2023-08-15 20:32:47,902: 64: pixi_hr_project_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2023-08-15 20:32:47,902: 17: pixi_hr_project_logger: INFO: 369255095:  Fetching Data Validation Configuration...]\n",
      "[2023-08-15 20:32:47,903: 64: pixi_hr_project_logger: INFO: common:  Created directory at: artifacts/data_validation]\n",
      "[2023-08-15 20:32:47,903: 21: pixi_hr_project_logger: INFO: 369255095:  Initializing DataValidation Component...]\n",
      "[2023-08-15 20:32:47,926: 25: pixi_hr_project_logger: INFO: 369255095:  Validating Columns...]\n",
      "dict_keys(['date_of_job_post', 'title', 'job_location', 'company_name', 'job_link', 'job_summary', 'job_type', 'job_qualifications', 'job_description'])\n",
      "[2023-08-15 20:32:47,927: 33: pixi_hr_project_logger: INFO: 441901800:  All expected columns are present in the dataframe.]\n",
      "[2023-08-15 20:32:47,928: 29: pixi_hr_project_logger: INFO: 369255095:  All columns successfully validated.]\n",
      "[2023-08-15 20:32:47,928: 33: pixi_hr_project_logger: INFO: 369255095:  Data Validation Pipeline completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "from pixi_hr import logger\n",
    "from pixi_hr.config.configuration import ConfigurationManager\n",
    "from pixi_hr.components.data_validation import DataValidation\n",
    "\n",
    "class DataValidationTrainingPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for validating the data before training or processing. \n",
    "\n",
    "    This pipeline performs the following steps:\n",
    "    1. Initializes configuration management.\n",
    "    2. Fetches the data validation configuration.\n",
    "    3. Initializes the DataValidation component using the fetched configuration.\n",
    "    4. Validates the columns of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Data Validation Stage\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidationTrainingPipeline.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Executes the main functionality of the DataValidationTrainingPipeline.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting the Data Validation Pipeline...\")\n",
    "\n",
    "        # Step 1: Initialize ConfigurationManager\n",
    "        logger.info(\"Initializing ConfigurationManager...\")\n",
    "        config = ConfigurationManager()\n",
    "\n",
    "        # Step 2: Fetch Data Validation Configuration\n",
    "        logger.info(\"Fetching Data Validation Configuration...\")\n",
    "        data_validation_config = config.get_data_validation_config()\n",
    "\n",
    "        # Step 3: Initialize DataValidation Component\n",
    "        logger.info(\"Initializing DataValidation Component...\")\n",
    "        data_validation = DataValidation(config=data_validation_config)\n",
    "\n",
    "        # Step 4: Validate Columns\n",
    "        logger.info(\"Validating Columns...\")\n",
    "        validation_status = data_validation.validate_columns()\n",
    "\n",
    "        # Log the result of the validation\n",
    "        if validation_status:\n",
    "            logger.info(\"All columns successfully validated.\")\n",
    "        else:\n",
    "            logger.warning(\"Column validation failed. Check logs for more details.\")\n",
    "        \n",
    "        logger.info(\"Data Validation Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> Stage: {DataValidationTrainingPipeline.STAGE_NAME} started <<<<<<\")\n",
    "        data_validation_training_pipeline = DataValidationTrainingPipeline()\n",
    "        data_validation_training_pipeline.main()\n",
    "        logger.info(f\">>>>>> Stage {DataValidationTrainingPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error encountered during the Data Validation Pipeline: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from pixi_hr import logger\n",
    "\n",
    "class DataValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def validate(self):\n",
    "        self.validate_columns()\n",
    "        self.validate_date_of_job_post()\n",
    "        self.validate_text_fields(['title', 'job_location', 'company_name', 'job_link', 'job_summary', 'job_description'])\n",
    "        self.validate_job_link()\n",
    "        self.validate_job_type()\n",
    "        self.validate_job_qualifications()\n",
    "        self.handle_duplicates()\n",
    "\n",
    "    def validate_columns(self):\n",
    "        \"\"\"\n",
    "        Ensure that all expected columns are present in the dataset.\n",
    "        \"\"\"\n",
    "        expected_columns = [\n",
    "            'date_of_job_post', 'title', 'job_location', 'company_name', \n",
    "            'job_link', 'job_summary', 'job_type', 'job_qualifications', \n",
    "            'job_description'\n",
    "        ]\n",
    "        missing_columns = set(expected_columns) - set(self.df.columns)\n",
    "        extra_columns = set(self.df.columns) - set(expected_columns)\n",
    "        \n",
    "        if missing_columns:\n",
    "            logger.warning(f\"Missing columns: {missing_columns}\")\n",
    "        if extra_columns:\n",
    "            logger.warning(f\"Extra columns found: {extra_columns}\")\n",
    "        if not missing_columns and not extra_columns:\n",
    "            logger.info(\"All expected columns are present in the dataframe.\")\n",
    "\n",
    "    def validate_date_of_job_post(self):\n",
    "        \"\"\"\n",
    "        Validate that 'date_of_job_post' contains valid date-time strings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df['date_of_job_post_temp'] = pd.to_datetime(self.df['date_of_job_post'], errors='raise')\n",
    "            logger.info(\"All values in 'date_of_job_post' are valid date-time strings.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error encountered: {e}\")\n",
    "            self.df['date_of_job_post_temp'] = pd.to_datetime(self.df['date_of_job_post'], errors='coerce')\n",
    "            invalid_rows = self.df[self.df['date_of_job_post_temp'].isna()]\n",
    "            logger.warning(\"\\nRows with invalid date-time strings:\")\n",
    "            logger.warning(invalid_rows[['date_of_job_post']])\n",
    "\n",
    "    def validate_text_fields(self, columns):\n",
    "        \"\"\"\n",
    "        Validate that the specified columns contain only text values.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            non_text_rows = self.df[self.df[column].apply(lambda x: not isinstance(x, str))]\n",
    "            if not non_text_rows.empty:\n",
    "                logger.warning(f\"Column '{column}' has non-text values:\")\n",
    "                logger.warning(non_text_rows[[column]])\n",
    "            else:\n",
    "                logger.info(f\"Column '{column}' contains only text values.\")\n",
    "\n",
    "    def validate_job_link(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_link' starts with \"http\".\n",
    "        \"\"\"\n",
    "        invalid_urls = self.df[~self.df['job_link'].str.startswith(\"http\")]\n",
    "        if not invalid_urls.empty:\n",
    "            logger.warning(f\"Found {len(invalid_urls)} rows with URLs not starting with 'http':\")\n",
    "            logger.warning(invalid_urls[['job_link']])\n",
    "        else:\n",
    "            logger.info(\"All URLs in 'job_link' start with 'http'.\")\n",
    "\n",
    "    def validate_job_type(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_type' contains only text values, excluding nulls.\n",
    "        \"\"\"\n",
    "        non_null_job_types = self.df[self.df['job_type'].notna()]\n",
    "        non_text_job_types = non_null_job_types[~non_null_job_types['job_type'].apply(lambda x: isinstance(x, str))]\n",
    "        if not non_text_job_types.empty:\n",
    "            logger.warning(f\"Found {len(non_text_job_types)} rows in 'job_type' with non-text values:\")\n",
    "            logger.warning(non_text_job_types[['job_type']])\n",
    "        else:\n",
    "            logger.info(\"All non-null values in 'job_type' are of text type.\")\n",
    "\n",
    "    def validate_job_qualifications(self):\n",
    "        \"\"\"\n",
    "        Validate that 'job_qualifications' contains valid lists of text values.\n",
    "        \"\"\"\n",
    "        def is_valid_list(value):\n",
    "            try:\n",
    "                lst = ast.literal_eval(value)\n",
    "                return isinstance(lst, list) and all(isinstance(i, str) for i in lst)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return False\n",
    "\n",
    "        invalid_qualifications = self.df[~self.df['job_qualifications'].apply(is_valid_list)]\n",
    "        if not invalid_qualifications.empty:\n",
    "            logger.warning(f\"Found {len(invalid_qualifications)} rows in 'job_qualifications' with invalid format:\")\n",
    "            logger.warning(invalid_qualifications[['job_qualifications']])\n",
    "        else:\n",
    "            logger.info(\"All values in 'job_qualifications' are valid lists of text values.\")\n",
    "\n",
    "    def handle_duplicates(self):\n",
    "        \"\"\"\n",
    "        Handle duplicate rows based on the 'job_link' column.\n",
    "        \"\"\"\n",
    "        num_duplicates = self.df[self.df['job_link'].duplicated()].shape[0]\n",
    "        if num_duplicates > 0:\n",
    "            self.df.drop_duplicates(subset='job_link', inplace=True)\n",
    "            logger.info(f\"Dropped {num_duplicates} duplicate rows based on the 'job_link' column.\")\n",
    "        else:\n",
    "            logger.info(\"No duplicates found based on the 'job_link' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-15 16:55:51,652: 8: pixi_hr_project_logger: INFO: 211043931:  Starting data validation process.]\n",
      "[2023-08-15 16:55:51,654: 33: pixi_hr_project_logger: WARNING: 3972941761:  Extra columns found: {'date_of_job_post_temp'}]\n",
      "[2023-08-15 16:55:51,658: 43: pixi_hr_project_logger: INFO: 3972941761:  All values in 'date_of_job_post' are valid date-time strings.]\n",
      "[2023-08-15 16:55:51,659: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'title' contains only text values.]\n",
      "[2023-08-15 16:55:51,660: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'job_location' contains only text values.]\n",
      "[2023-08-15 16:55:51,661: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'company_name' contains only text values.]\n",
      "[2023-08-15 16:55:51,661: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'job_link' contains only text values.]\n",
      "[2023-08-15 16:55:51,662: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'job_summary' contains only text values.]\n",
      "[2023-08-15 16:55:51,663: 61: pixi_hr_project_logger: INFO: 3972941761:  Column 'job_description' contains only text values.]\n",
      "[2023-08-15 16:55:51,664: 72: pixi_hr_project_logger: INFO: 3972941761:  All URLs in 'job_link' start with 'http'.]\n",
      "[2023-08-15 16:55:51,665: 84: pixi_hr_project_logger: INFO: 3972941761:  All non-null values in 'job_type' are of text type.]\n",
      "[2023-08-15 16:55:51,669: 102: pixi_hr_project_logger: INFO: 3972941761:  All values in 'job_qualifications' are valid lists of text values.]\n",
      "[2023-08-15 16:55:51,669: 113: pixi_hr_project_logger: INFO: 3972941761:  No duplicates found based on the 'job_link' column.]\n",
      "[2023-08-15 16:55:51,698: 23: pixi_hr_project_logger: INFO: 211043931:  Validated dataframe saved to artifacts/data_validation/validated_data.csv]\n",
      "[2023-08-15 16:55:51,699: 25: pixi_hr_project_logger: INFO: 211043931:  Data validation process completed.]\n"
     ]
    }
   ],
   "source": [
    "def main(dataframe):\n",
    "    \"\"\"\n",
    "    Main function to validate the dataframe.\n",
    "    \n",
    "    Args:\n",
    "    - dataframe (pd.DataFrame): The dataframe to validate.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data validation process.\")\n",
    "    \n",
    "    # Initialize the DataValidator class with the dataframe\n",
    "    validator = DataValidator(dataframe)\n",
    "    \n",
    "    # Run the validations\n",
    "    validator.validate()\n",
    "    \n",
    "    # Ensure the directory exists or create it\n",
    "    save_dir = \"artifacts/data_validation\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the validated dataframe\n",
    "    save_path = os.path.join(save_dir, \"validated_data.csv\")\n",
    "    dataframe.to_csv(save_path, index=False)\n",
    "    logger.info(f\"Validated dataframe saved to {save_path}\")\n",
    "    \n",
    "    logger.info(\"Data validation process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample usage: (Replace 'df' with your actual dataframe variable)\n",
    "    main(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_hr_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from urllib.parse import quote_plus, urlparse, ParseResult\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr/research'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Desktop/pixi_hr_project/pixi_hr'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install cloudscraper\n",
    "# !pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourcing:\n",
    "    \"\"\"\n",
    "    A class used to scrape job postings from SimplyHired based on specific job titles and locations.\n",
    "\n",
    "    The class supports scraping multiple job titles across multiple locations and will only scrape new job postings\n",
    "    not present in an existing dataset.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    base_url : str\n",
    "        The base URL for SimplyHired job search (default is \"https://www.simplyhired.ca\").\n",
    "    job_searches : list\n",
    "        A list of job titles to search for (default is [\"Data Scientist\"]).\n",
    "    locations : list\n",
    "        A list of locations to search within (default is [\"Toronto\"]).\n",
    "    existing_job_links : list\n",
    "        A list of job links already present in the dataset, used to prevent redundant scraping.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    _load_existing_job_links():\n",
    "        Load job links from the existing dataset and return them.\n",
    "    _get_headers():\n",
    "        Generate random headers for web requests to mimic real browser requests.\n",
    "    ensure_directory_exists(filepath):\n",
    "        Ensure the directory for the given filepath exists. If not, create it.\n",
    "    _construct_url(job_search, location, page=1):\n",
    "        Construct the URL based on the job search query, location, and page number.\n",
    "    scrape_jobs():\n",
    "        Generator function that yields job data for each page and job, excluding jobs already present in the dataset.\n",
    "    _get_soup(url):\n",
    "        Fetch and parse the HTML content of a given URL.\n",
    "    _extract_job_data(job):\n",
    "        Extract essential job details from a BeautifulSoup job element and return as a dictionary.\n",
    "    _get_job_details(job_url):\n",
    "        Fetch and return additional job details for a given job URL.\n",
    "    main():\n",
    "        Main function to scrape job data, fetch job details, and save to CSV.\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, base_url=\"https://www.simplyhired.ca\", job_searches=[\"Data Scientist\"], locations=[\"Toronto\"]):\n",
    "        \"\"\"\n",
    "        Initialize the DataSourcing object.\n",
    "\n",
    "        Parameters:\n",
    "        - base_url (str): The base URL for the job search.\n",
    "        - job_searches (list): List of job titles to search for.\n",
    "        - locations (list): List of locations to search within.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.job_searches = job_searches\n",
    "        self.locations = locations\n",
    "        self.existing_job_links = self._load_existing_job_links()\n",
    "\n",
    "        # More attributes can be added as needed\n",
    "\n",
    "    \n",
    "    def _load_existing_job_links(self):\n",
    "        \"\"\"Load job links from the existing dataset and return them.\n",
    "        \n",
    "        Returns:\n",
    "        - list: A list of existing job links.\n",
    "        \"\"\"\n",
    "        file_path = 'artifacts/sourcing/data_scientist_jobs.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            existing_jobs_df = pd.read_csv(file_path)\n",
    "            return existing_jobs_df['job_link'].tolist()\n",
    "        return []\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _get_headers():\n",
    "        \"\"\"Generate random headers for web requests to mimic real browser requests.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: Dictionary of headers.\n",
    "        \"\"\"\n",
    "\n",
    "        # A list of popular user-agents to mimic real browser requests.\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\"\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        }\n",
    "\n",
    "        return headers\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_directory_exists(filepath):\n",
    "        \"\"\"Ensure the directory for the given filepath exists. If not, create it.\n",
    "\n",
    "        Parameters:\n",
    "        - filepath (str): The path to check.\n",
    "        \"\"\"\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "    def _construct_url(self, job_search, location, page=1):\n",
    "        \"\"\"\n",
    "        Construct the URL based on the job search query, location, and page number.\n",
    "\n",
    "        Parameters:\n",
    "        - job_search (str): The job title to search for.\n",
    "        - location (str): The location to search within.\n",
    "        - page (int): The page number.\n",
    "\n",
    "        Returns:\n",
    "        - str: Constructed URL.\n",
    "        \"\"\"\n",
    "        job_query = quote_plus(job_search)\n",
    "        location_query = quote_plus(location)\n",
    "        return f\"{self.base_url}/search?q={job_query}&l={location_query}&pn={page}\"\n",
    "    \n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        \"\"\"\n",
    "        Generator function that yields job data for each page and job, excluding jobs already present in the dataset.\n",
    "\n",
    "        Yields:\n",
    "        - dict: Dictionary containing job details.\n",
    "        \"\"\"\n",
    "        soup = self._get_soup(self._construct_url(self.job_search, self.location))\n",
    "        total_job_count = int(soup.find('span', {'class': 'posting-total'}).text.replace(',', ''))\n",
    "        job_pages = math.ceil(total_job_count/20)\n",
    "\n",
    "        for page in range(1, job_pages + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            soup = self._get_soup(self._construct_url(self.job_search, self.location, page))\n",
    "            for job in soup.find('ul', {'class': 'jobs'}).findAll('div', {'class': 'SerpJob-jobCard'}):\n",
    "                job_data = self._extract_job_data(job)\n",
    "                # Skip the job if it's already in the existing dataset\n",
    "                if job_data['job_link'] in self.existing_job_links:\n",
    "                    continue\n",
    "                yield job_data\n",
    "\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        \"\"\"Fetch and parse the HTML content of a given URL.\n",
    "\n",
    "        Parameters:\n",
    "        - url (str): The URL to fetch.\n",
    "\n",
    "        Returns:\n",
    "        - BeautifulSoup object: Parsed HTML content.\n",
    "        \"\"\"\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        headers = self._get_headers()\n",
    "        response = scraper.get(url, headers=headers)\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_job_data(job):\n",
    "        \"\"\"Extract essential job details from a BeautifulSoup job element and return as a dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - job (BeautifulSoup element): The job element to extract data from.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary containing essential job details.\n",
    "        \"\"\"\n",
    "        title_tag = job.find('h3', {'class': 'jobposting-title'})\n",
    "        title = title_tag.text\n",
    "        link = title_tag.find('a').attrs['data-mdref']\n",
    "        parsed_link = urlparse(link)\n",
    "        link = ParseResult(scheme=parsed_link.scheme, netloc=parsed_link.netloc, \n",
    "                           path=parsed_link.path, params=parsed_link.params, \n",
    "                           query='', fragment=parsed_link.fragment).geturl()\n",
    "\n",
    "        return {\n",
    "            'date_of_job_post': job.find('time').attrs['datetime'],\n",
    "            'title':  title,\n",
    "            'job_location': job.find('span', {'class': 'jobposting-location'}).text,\n",
    "            'company_name': job.find('span', {'class': 'jobposting-company'}).text,\n",
    "            'job_link': f\"https://www.simplyhired.ca{link}\",\n",
    "            'job_summary': job.find('p', {'class': 'jobposting-snippet'}).text\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "    def _get_job_details(self, job_url):\n",
    "        \"\"\"Fetch and return additional job details for a given job URL.\n",
    "\n",
    "        Parameters:\n",
    "        - job_url (str): The URL of the job to fetch details for.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary containing additional job details.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a new scraper for each job detail page\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        headers = self._get_headers()\n",
    "        response = scraper.get(job_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract additional info, like job description, job type, qualifications, etc.\n",
    "        job_details = soup.find('div', {'class': 'viewjob-content'})\n",
    "\n",
    "        # Job type\n",
    "        job_type_tag = job_details.find('span', {'class': 'viewjob-jobType'})\n",
    "        job_type = job_type_tag.text if job_type_tag else None  # Default to \"N/A\" if tag is not found\n",
    "\n",
    "        # Job qualifications\n",
    "        qualifications_tags = job_details.findAll('li', {'class': 'viewjob-qualification'})\n",
    "        job_qualifications = [qual_tag.text for qual_tag in qualifications_tags] if qualifications_tags else None\n",
    "\n",
    "        # Job description\n",
    "        job_description_tag = job_details.find('div', {'data-testid': 'VJ-section-content-jobDescription'})\n",
    "        job_description = job_description_tag.text if job_description_tag else None\n",
    "\n",
    "        return {\n",
    "            'job_type': job_type,\n",
    "            'job_qualifications': job_qualifications,\n",
    "            'job_description': job_description\n",
    "        }\n",
    "    \n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Main function to:\n",
    "        1. Scrape job data.\n",
    "        2. Fetch job details.\n",
    "        3. Save to CSV, appending to existing data and ensuring no duplicates.\n",
    "        \"\"\"\n",
    "        all_jobs = []\n",
    "\n",
    "        # Iterate over each combination of job title and location\n",
    "        for job_search in self.job_searches:\n",
    "            for location in self.locations:\n",
    "                self.job_search = job_search  # Update the current job search\n",
    "                self.location = location      # Update the current location\n",
    "\n",
    "                job_data_generator = self.scrape_jobs()\n",
    "                jobs_df = pd.DataFrame(job_data_generator)\n",
    "\n",
    "                with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                    job_details_list = list(executor.map(self._get_job_details, jobs_df['job_link'].tolist()))\n",
    "\n",
    "                jobs_details_df = pd.DataFrame(job_details_list)\n",
    "                all_jobs.append(pd.concat([jobs_df, jobs_details_df], axis=1))\n",
    "\n",
    "        # Combine all job data\n",
    "        all_jobs_df = pd.concat(all_jobs, ignore_index=True)\n",
    "\n",
    "        file_path = 'artifacts/sourcing/data_scientist_jobs.csv'\n",
    "\n",
    "        # Read existing data if file exists\n",
    "        if os.path.exists(file_path):\n",
    "            existing_jobs_df = pd.read_csv(file_path)\n",
    "            all_jobs_df = pd.concat([existing_jobs_df, all_jobs_df], ignore_index=True)\n",
    "\n",
    "        # Ensure the directory exists before saving the file\n",
    "        self.ensure_directory_exists(file_path)\n",
    "        \n",
    "        all_jobs_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 1...\n",
      "Scraping page 1...\n",
      "Scraping page 1...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "Scraping page 52...\n",
      "Scraping page 53...\n",
      "Scraping page 54...\n",
      "Scraping page 55...\n",
      "Scraping page 56...\n",
      "Scraping page 57...\n",
      "Scraping page 58...\n",
      "Scraping page 59...\n",
      "Scraping page 60...\n",
      "Scraping page 61...\n",
      "Scraping page 62...\n",
      "Scraping page 63...\n",
      "Scraping page 64...\n",
      "Scraping page 65...\n",
      "Scraping page 66...\n",
      "Scraping page 67...\n",
      "Scraping page 68...\n",
      "Scraping page 69...\n",
      "Scraping page 70...\n",
      "Scraping page 71...\n",
      "Scraping page 72...\n",
      "Scraping page 73...\n",
      "Scraping page 74...\n",
      "Scraping page 75...\n",
      "Scraping page 76...\n",
      "Scraping page 77...\n",
      "Scraping page 78...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    job_titles = [\"Data Scientist\", \"Data Engineer\", \"Machine Learning Engineer\"]\n",
    "    cities = [\"Toronto\", \"Vancouver\", \"Montreal\"]\n",
    "    \n",
    "    data_sourcing = DataSourcing(job_searches=job_titles, locations=cities)\n",
    "    data_sourcing.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"artifacts/data_transformation/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_of_job_post    1976\n",
       "title               1976\n",
       "job_location        1976\n",
       "company_name        1976\n",
       "job_link            1976\n",
       "                    ... \n",
       "qual_xpath          1976\n",
       "qual_xslt           1976\n",
       "qual_xunit          1976\n",
       "qual_zbrush         1976\n",
       "qual_zookeeper      1976\n",
       "Length: 556, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_of_job_post    1\n",
       "title               1\n",
       "job_location        1\n",
       "company_name        1\n",
       "job_link            1\n",
       "                   ..\n",
       "qual_xpath          1\n",
       "qual_xslt           1\n",
       "qual_xunit          1\n",
       "qual_zbrush         1\n",
       "qual_zookeeper      1\n",
       "Length: 556, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values in the DataFrame\n",
    "nan_columns = df.columns[df.isna().any()]\n",
    "nan_count = df[nan_columns].isna().sum()\n",
    "\n",
    "# Display columns with NaN values and their respective counts\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nan = df[df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_of_job_post</th>\n",
       "      <th>title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_description</th>\n",
       "      <th>date_of_job_post_temp</th>\n",
       "      <th>month_of_job_post</th>\n",
       "      <th>...</th>\n",
       "      <th>qual_woodworking</th>\n",
       "      <th>qual_wordpress</th>\n",
       "      <th>qual_workday</th>\n",
       "      <th>qual_writingskills</th>\n",
       "      <th>qual_xml</th>\n",
       "      <th>qual_xpath</th>\n",
       "      <th>qual_xslt</th>\n",
       "      <th>qual_xunit</th>\n",
       "      <th>qual_zbrush</th>\n",
       "      <th>qual_zookeeper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2023-08-13 01:03:15+00:00</td>\n",
       "      <td>169.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>https://www.simplyhired.ca/job/viByG-pZnRAAaKb...</td>\n",
       "      <td>Dans le cadre de ses ententes avec ses différe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dans le cadre de ses ententes avec ses différe...</td>\n",
       "      <td>2023-08-13 01:03:15+00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date_of_job_post  title  job_location  company_name  \\\n",
       "76    2023-08-13 01:03:15+00:00  169.0          22.0         727.0   \n",
       "1161                        NaN    NaN           NaN           NaN   \n",
       "\n",
       "                                               job_link  \\\n",
       "76    https://www.simplyhired.ca/job/viByG-pZnRAAaKb...   \n",
       "1161                                                NaN   \n",
       "\n",
       "                                            job_summary  job_type  \\\n",
       "76    Dans le cadre de ses ententes avec ses différe...       0.0   \n",
       "1161                                                NaN       NaN   \n",
       "\n",
       "                                        job_description  \\\n",
       "76    Dans le cadre de ses ententes avec ses différe...   \n",
       "1161                                                NaN   \n",
       "\n",
       "          date_of_job_post_temp  month_of_job_post  ...  qual_woodworking  \\\n",
       "76    2023-08-13 01:03:15+00:00                8.0  ...               NaN   \n",
       "1161                        NaN                NaN  ...               0.0   \n",
       "\n",
       "      qual_wordpress  qual_workday  qual_writingskills  qual_xml  qual_xpath  \\\n",
       "76               NaN           NaN                 NaN       NaN         NaN   \n",
       "1161             0.0           0.0                 0.0       0.0         0.0   \n",
       "\n",
       "      qual_xslt  qual_xunit  qual_zbrush  qual_zookeeper  \n",
       "76          NaN         NaN          NaN             NaN  \n",
       "1161        0.0         0.0          0.0             0.0  \n",
       "\n",
       "[2 rows x 556 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_with_nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixi_hr_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
